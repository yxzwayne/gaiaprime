- The training and testing sequences are synthetic, they are generated by a Potts model sufficiently trained on a family of natural sequences, let's call it Potts A. I then train an entirely new Potts model, let's call it Potts B, based on the synthetic dataset. We assume that Potts B is a baseline for our VAE performance, because Potts model is mechanically designed to only capture pairwise interactions.
- I train the VAE model on these sequences. I then generate equal amount of test sequences from both the Potts model and the VAE model, as well as an independent model that assumes not even pairwise interactions in sequences, given a fixed set of test input sequences. I use the independent model as baseline for both Potts and VAEs, you should probably be able to tell why we can do this.
- How do I measure whether the VAE can model higher-order interactions? Through a metric called the r20 metric, which tests the generative capability of a model. To obtain this metric, I randomly designate positions (or residues) within the valid protein sequence length, pull the amino acid letter representations out, form subsequences with these pulled-out residues, and calculate the spearman correlations between the Potts B or VAE subsequences and the independent sequences. so the comparison will be like Potts B vs. Indep and VAE vs Indep.
- After obtaining the spearman score for both Potts B and VAE, I compare them for each subsequence length as determined by the number of residues i pull out. 
This experiment is relevant to why I want to finish the tempered posterior paper, because our input training data is synthetic! it is sampled from Potts A! However, this is an entirely different data curation problem compared to the image dataset selection via human labeling. Also, the problem is different, because the paper discusses classification, but we are generating new sequences. 
In order for me to be able to search, reason and formulate any association or relevance between the paper and my work, what should we think about?
