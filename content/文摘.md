---
title: 文摘
---

2022年冬，我在临沂城送外卖
作者：邢斌 临沂大学文学院教授

一、外卖的基本情况说干就干。最简单的是跑“美团众包”: 不用培训，注册就行，不限时间，不规定最低工作量，收入一天一结。还有“蜂鸟众包(以前是饿了吗)。基本同上。后来又注册了“闪送”(据说这家最规范最人性化)。去齐鲁园培训了半天，花了50块钱买了工牌、马甲、文件袋。“顺丰同城”，和“闪送”差不多，就不再体验了，否则还得花一份钱。这几家相比较，美团最狠，市场占有率最高。蜂鸟次之，闪送和顺丰相对温和但生意不多。美团把骑手分三个等级。核心是美团专送，职业骑手。每天九点打卡上班，晚上九点下班。埋头跑系统推送的单子，每单3-4元。都是优化过的好单。取餐处集中，比如说上万达四楼，一圈拿5-6个餐。派送处集中，比如说送到某银行前台一次放5-6份餐。派送距离短，不超过3公里。派送时间短，因为取餐省时间、放餐可以直接放前台，不用进小区、上电梯、爬楼、给顾客反复打电话确认。他们相对轻松一些。但不好处是不准请假，每个月允许歇四天，歇哪天得提前一星期报备，越是刮风下雨下雪越要求准时派送。接到差评罚款200-500，一般罚500。送餐迟到不罚款因为是系统计算出来的，人只管快跑就是。说是早九点干到晚九点，实际上还要长。上午八点半开早会，迟到一次扣20。晚上到九点了，手里的餐送完才能打卡回家 (一般到八点半还会继续派单，多数都是九点半才能打卡回家)。在临沂城，专送每天必须干12-14小时，一个月必须干26-28天，平均能挣6000。特别拼命的能挣8000多，都是市区60码逆行闯红灯拿命换的。专送很苦，但业余送外卖的众包骑手更苦，处于最低的第三级。送一趟单价低30%，单子都是专送挑剩下的，不是偏就是远，要么就是要去没有电梯的搬迁小区爬六楼送上门。好单很难抢到 (上两层有60-30秒的提前抢单优先权)。众包工作时间更长，更危险，挣的更少，不听话就被“针对性”礼送到没有订单的边缘地区。好处是不想干就回家躺着，没人强迫你挣钱。我了解的最拼命的，每个月能挣7000 (每天干15-16个小时，一个月一天不歇)。我有本职工作，只能干业余的众包。这些天我假日就从早干到晚，工作日早晨跑两小时，晚饭后再跑到夜里一两点，或者两三点。夜里给钱多一些，能挣到钱的都是偏远地方。半夜里我跑到过相公镇东边的村里，跑到方城，跑到兰陵村子里，跑到沂南山里面，都是乡间小路。都是骑摩托。太黑，灯书院照不远，有时候就骑到了沟里。一过长春路，夜里都是大货车，擦身而过，心里也打怵。送完货，骑车回来，才觉得手麻觉得冻得不行。有一次我实在太冷了，就把车停在田里，绕着跑了一阵，看看高德地图，在临沂大学正北九公里，回家还很漫长。一个月，我送了2000多单，接触了几百个商家，敲响了2000多个房门。平均下来每天骑摩托210公里、步行32000步、爬110层楼。所以我那个月微信运动里每天都是步行最多的。这个月综合算下来，每小时收入10元是常态，每小时收入20元是极限。平均每单3.5元，要取货送货2+3公里，取货平均等5分钟，骑车8分钟，送货进小区上门平均7分钟，共20分钟。一小时3单,10.5元。一次送3单，排列好次序，能节省1/3时间。但会被催。一小时能送4-5单，15.75元。一次送5单，适用于午餐晚餐的集中送餐时间。很难排列好次序不被催促。基本上就是极限值。略微提升单位时间的收入，一小时还是送5-6单，19.25元。好处是能最大化高峰期的送餐量。关于送的货品，我有一些小的建议: 蛋糕不建议送。鲜花，不建议送。冬天不要送烧烤。万达、泰盛，不建议取货。医院，不建议送。代买，不建议送。菜市场代买，坚决不送。啤酒，最好不送。转单，不能接。一超市用品，比送餐好。文具、药品，最佳。轻便而不易损坏的特殊用品，建议送。长途，加急，谨慎判断。二、痛苦，是一件事实，还是一种体验?诗歌只有一种现实: 痛苦——帕切科《诗人之恋》我无心于调查，就是想体验体验。2022年很特殊。一份报道里讲，在上海骑手送外卖每天能赚1000多。另一份报道又说，北京人社局一位副处长，王林，亲身体验当外卖骑手，送餐12小时赚了41元。究竟哪个说真的?我想，应该亲身试试才知道真伪。干了一周，我觉得王林处长那篇报道更真实。大家有时间可以搜一下看看他的具体讲述。不过，我觉得12小时赚41元，这根本不能维持生活，何况在北京。王林处长体验得有些短，他要是干上个把月，我估计每个月能赚个三四千块钱。要不，他怎么活？我体验了一个月。这张纸上是我每天记录的收入情况 (没有去除每天3元保险费和25元摩托车油费)。跑到第20天的时候，我上升到了众包骑手的最高级别，路也很熟了(临沂市区的小区和周边的乡镇都牢记了,不需要看导航。经常去的小区，我每天回家都默背一阵具体楼号编排次序，提高步行送餐速度)，基本上算是很熟练的骑手了。劳动强度和具体收入情况，大致如此。但我主要关注的是外卖员这个“身份”究竟是怎样的处境。体验这个工作过程中人是如何感受、应对、反刍这些遭遇的。肉体受罪是一方面，虽然很久没有这种体验了；主要的还是受人辱骂。没有人拿正眼看送外卖的，商家，顾客，尤其是保安。熟人都不知道我最近在干这个，只有我们小区的保安知道。他们天天见我早出晚归半夜回来，不让我进。我说我是业主;他们骑车跟着我到楼下看我上楼，说你送外卖都能在这儿买起房，是个人物。很多顾客看我的眼神就像看要饭的。有的顾客就是披着人皮的畜生。我曾经痛恨过这些人，后来慢慢都忘了。有几次，差点就被人认出来了。一次是在万达。我取餐的时候，旁边有一对小情侣说那个送外卖的说话好像邢老师啊。我心想肯定是爱听我课的好孩子。还有一次，我半夜送螺狮粉到宝德新领域，开门的男士穿着内衣，他是我的前同事。我认出了他，他肯定没认出我。我戴着头盔。我更愿意回忆起温暖的瞬间。这一个月，我送了两千多单，有三个人真诚地感谢过我。一个是搬迁小区古城社区的一位女士。她说半夜里孩子想吃馄饨，天这么冷谢谢我专程送来。后来我发现她又打赏了我2块钱。还有一位，也是女士，就是相公镇东北那个村里的。他们夫妻俩怕我半夜里找不着路，打着手电把我送到了路口。还有一对夫妻，老人住在人民医院五号楼，心脑血管疾病中心。我把他们给老人定的餐洒了一些，后来我又买了一份送去。他们俩把第二份餐的钱退给我，又打赏了我10块钱。真的很感谢他们。祝愿他们事事顺利，吉祥如意。三、今天，我们体面地存在于社会中,究竟需要每月多少“成本”？在我栖息的孤独中有充裕的时间来思考希望的问题：能否有一天我们的生命不再像霍布斯所说只是污秽、野蛮与短暂的?——帕切科《约拿报告》我们看一下中国这几家外卖公司的隐形控制结构：外卖公司总部把所有城市都分包给每个城区的运营商，然后运营商再次分包。这是一种非常特殊的结构，很多国家是不允许这样操作的。肯德基、麦当劳它们都有自己的外卖队伍: 无论专职还是业余，都有正式签约.有五险一金，受伤有公司保险，从不设置送餐倒计时催促你飞马赶到否则亏款罚款。——我们这几家公司 (在此我不便说它们的名字)，实际情况就是，骑手出车祸了，每天扣的3元保险 (公司扣了60%，只把1.2元交给保险公司) 提供最高6000块钱的伤亡保险。不够了，县区运营商承担。还不够治病，县区运营商直接跑路，你起诉都找不到人。起诉城市的运营商都起诉不了，外卖公司总部根本起诉不着，因为都是“劳动外包”，它把自己早隔离出去了。这种重大伤亡事故，据了解城区每个月都有。猝死，外卖公司总部所有阶层的管理都不会——按照他们的条款，这与外卖公司理睬，总部毫无关系，起诉都是白花钱。全国起诉的都没一个赢的。罚款的问题。——顾客投诉，会被重罚。这个网上讨论很多，我就不赘述了。我说下另个关于罚款的问题。比如说：案例一。这一单完成得很好，很完美。然后继续送的过程中，软件提示说上一个订单没有点击送达。你只好停下了点击。第二天就会被罚款：异地点击送达，或者超时点击送达。可以投诉说，没有超时，而且当时已经点击了送达，这是软件又跳出来的问题。投诉无效。还有一次申诉机会。再次申诉也秒回，无效。打人工客服电话，一个小时内能联系上就是幸运的。联系上了，还是同样的回复。这样的情况我一个月遇到两次，程序完全一样，三次申诉机会没一点点用，完全是摆设。我最后对人工客服(是个活人) 说，你们可以直接联系顾客看真实情况是怎样的。没用，依旧扣钱。而且扣的钱也不返还顾客，都进了外卖公司总部腰包。案例二：跑腿单。送达过程顺利，顾客非常满意。但是顾客不会在手机上完成“垫付款。顾客找不到如何支付垫付的页面，骑手就得在门外等着，也不好大声催促。(大声催促是态度不好，是要被顶格罚款的，罚500) 。一家人在屋内找垫付款入口。等了好久，系统提示说送达已经超时。超时一秒钟，扣跑腿费用40%。超时五分钟，扣跑腿费60%。昨天我在楼梯上等了快十分钟，顾客才完成支付。今天显示那一单扣款80%白送了，从大学城附近到罗庄，11公里。有一次，在小海螺，替顾客代付餐费后等餐(四菜一汤)，等了半小时第一份菜还没做，打电话给经理报备，回复说继续等待等到45分钟还没出菜，再打电话给经理报备，回复说倒计时宽限15分钟；等到倒计时都快走完了，我已经等了75分钟，再打电话给经理报备，回复说和顾客商量，尽量让顾客满意别投诉。顾客很体谅，说: 既然已经付款了，等一等就再等一等吧，快春节了，人多，理解；我先在家把收货确认了，你安心送来就行。结果第二天一起床，就看到红色警告: 严重违规，罚款200元！怎么办，开始申诉呗。从第一级开始申诉，填单、录音、截图上报。被打回，申诉无效。第二级申诉....第三级申诉....第四级申诉，到总部了，有人电话录音取证；最后还是申诉无效。最后到第五级，最高级，总部市场部总经理，上海，.....还是申诉无效，答复说系统显示顾客填写好评的时间，我的定位还在饭店。我说刚才发给你的顾客专门录音的情况说明、大堂经理的录音情况说明、顾客接餐到家的照片已经把事实讲清楚了。回复说不行。同时反问我，你申诉了快一整天了,有这时间，你跑一天外卖，也快赚200块钱了，干嘛这么轴?我在电话里对这位总经理说，你应该看过一部电影，叫《秋菊打官司》;这不是钱的问题。过了一会儿，他打电话过来，说发了一个红包给我，请接收。我点开，15元红包，留言说我个人理解你的经历，但罚款不能取消这15块钱算是我个人的一个人道主义补偿。——这几家全球知名的外卖公司，盈利能力真有这么紧张吗?这家公司在美国纳斯达克上市，有严格的季度财报和年度财报。我们打开看一下。它自创办以来，十几年几乎没有一年是盈利的。18年亏损1155亿，21年亏损235亿，22年亏损67亿。我看到这个财报，心里和大家一样，非常惊讶。因为每一单外卖，商家需要额外支付货品价格30%的送货费用，顾客需要支付每公里0.5元的送货费用。举个例子，午餐定一个20元的饭，3公里，顾客支付20+2=22元;商家拿到14元，快递员拿到3元，外卖公司拿到5元。大概抽成比例是这样。它究竟为何亏损如此之大呢?我们还是看财报。21年，它行政开支88亿、研发开支167亿。22年，它行政开支98亿、研发开支208亿。钱都从这里流走了。它的所有分公司都是外包出去的，总部平台需要多少行政人员大家可以统计一下。它的平台，就是一个手机APP，每年需要多少研发费用来支撑，大家也可以统计一下。有时候，精心修订过的数字会误导世界。还不如我们日常的体验。这些骑手干着全世界强度最大的外卖工作，拿着最低比例的收入;商家一批一批退出，不再接受它这么重的抽成;它的大股东们在全世界豪宅游艇转移资产……和这些财报数据显示的完全不符。送外卖的一个月里，我见到了3个女性外卖员，见到了几位年龄很大的外卖骑手，最大的一位对我说今年66岁了。他们承担不了每天14小时、全年无休这么大强度的劳动，我估计他们每个月能赚个3、4000块钱。春节过后，我离开了外卖队伍，但在路上我还是首先注意到他们的身影。最近几个月，我看到了越来越多的女性和老人骑手风驰电掣争分夺秒。可能再危险再苦再累，他们也离不开这3、4000块钱的收入。家里的孩子、病床上的老人、银行发来的房贷还款短信……都在提醒着他们: 跑起来，快些跑!有天夜里，我在彷河边上一家烧烤店门口蹲着，等老板出餐。旁边还蹲着好几个美团骑手。我问他，现如今啥活最苦? 他说，送外卖挣钱最苦，还有快递中心搞分拣也苦，搬家搬货也苦，扛地板砖上楼也苦。我问他，比老家种地苦不? 他说，当然比种地苦了，种地清闲，又不来钱，种屁的地。我问他，这几样比干建筑活苦不? 他说，当然比干建筑活苦了，千建筑活，大工一天二百，小工一天一百八到二百;但你能拿到钱不? 半年有活，半年没活，干到年底工头跑了，过年，过屁年。我准备把这几样都干一遍。每一样干几个月。给自己油头粉面的内心减减肥。是否有一种公司，不仅能让我赚钱糊口，还能教我们学点好的东西?这是我心中最大的疑问。我们这种极度内卷源自哪里，谁能告诉我?我查证了国外快递业的具体情况。日本送一单起价是32元 (人民币)，北美送一单起价是6美元。所以国外让外卖送到家是很贵的。(高级知识分子的薪水，目前日本是国内的2-3倍，北美也是国内的2-3倍。但底层劳动者的收入，目前日本是国内的8-12倍，北美是国内的10-15倍。) 而且，国外的通例是双方都可以差评投诉:骑手被差评五次，要暂停工作重新培训。顾客被骑手差评五次，会被系统封号一年，不能再点外卖，必须自己去取。双方如果投诉，系统提供的都是真人接听电话，2-6小时反馈意见。调解不了，骑手和顾客都可以拿着证据去法院起诉，也可以起诉公司。像起诉公司“违规罚款”、“歧视”这种官司，打赢了一辈子就财务自由了 (一般都会收到公司钱庭外和解) 。因为那些工会，不是一般的厉害，是非常非常厉害。而我们这些外卖总部的管理系统，与刚才说的那些人性化的公司比较，内核完全不同。我们这些公司很特殊，就是我们这个“大系统”的具体而微，基因完全一样。它的一切设计，在大数据和人工智能的加持下，变得更精密、更准确，“恰好”能获取适量的劳动者，“恰好”能让骑手们维持最基本的生活，让他们积累不下休养生息、以钱养钱的些微资本，像驴一样，被牢牢拴在这台磨上。这不就是齐格蒙特-鲍曼在《工作、消费主义和新穷人》里写的那样吗?“新的工厂系统需要的只是人的一部分，是身处复杂机器之中，如同没有灵魂的小齿轮一样工作的那部分。而人身上那些无用的部分，比如兴趣和雄心，还有天性中对自由的渴望，不仅与生产力无关，还会干扰生产需要的那些有用的部分。”这是关于后现代状况的分析，令人揪心。而我们遭遇的，是加强版，更令人揪心。究竟谁在阻碍我们过上有最低体面水准的生活?四、知识分子的“信息茧房复仇是世界的主旋律人犯我，我犯人，人再犯我我们永续这无尽循环——帕切科《牢笼》我不觉得知识分子是一个多么美好的词。——它就是个中性词，既不好，也不坏。我读了很多年书，读了很多书，结识了很多读书人。但我觉得读书越多，盲区越大，反而会生成一种鄙视日常世界的莫名奇妙的自负。底层人生活在底层的信息茧房;知识分子生活在知识分子的信息茧房。两者经常是不相通的。我从另一个角度谈谈这个问题。我来自于一个天主教家庭。读高中的时候，我顶着巨大压力，离开了教会的钳制。母亲经常叹息说，这些事你做得比教徒还认真，为什么不回到教会里?为什么呢? 我觉得“精华已尽皆堪弃”。这是围棋世界里的一句话。——我不入地狱谁入地狱?这是天主教信念的核心。有这个信念是最关键的。我见过很多自称严格遵循戒律的教徒。就算他们言行一致，我也不喜欢: 被动地屈服于某些戒律，内心却充满了私欲，这不是买椟还珠吗?一个可持续发展的世界，应该理解人的有限性，理解财富是流动不居的。最起码，得理解世界各阶层必须平衡发展，竭泽而渔必将鸡飞蛋打。说他们之所以慷韦伯谈到清教徒的“慷慨”慨，不仅仅因为道德追求，主要是认识到必须让渡一部分利润给他人，才能维持系统的稳定运行。说得远一点，世界的终点是“空”(空不是无)。我所理解的“空”，是生灭灭生，循环往复，不为某人某姓永远独存。获取大量金钱，有点意思，但也没太大意思。以前古人说，由俭入奢易，由奢入俭难。老百姓爱说，人就是懒骨头，能上不能下。确实是这样的。年近半百，我感觉自己越来越娇气，越来越矫情，越来越脾气坏，越来越没耐心。这样发展下去，是要下地狱的。张爱玲说，有年元宵节，胡兰成陪着她到上海郊区闲转悠，钻进一个棚子里听流浪剧团唱野戏。寒冬，那些女演员冻得红彤彤的，嗓子都冻哑了，就在幕布后面土堆上描眉换装。看了一会儿，胡兰成说走吧。张爱玲说，你走吧，我再看会儿。后来，张爱玲在美国回忆这件事。她说，我感到震撼，这才是真正有生命力的女人，再苦再穷，大世界天崩地裂，也挡不住她们活下去，就像野草一样。确实如此。

---

The era of Artificial Intelligence is here, and boy are people freaking out. Fortunately, I am here to bring the good news: AI will not destroy the world, and in fact may save it.
First, a short description of what AI is: The application of mathematics and software code to teach computers how to understand, synthesize, and generate knowledge in ways similar to how people do it. AI is a computer program like any other – it runs, takes input, processes, and generates output. AI’s output is useful across a wide range of fields, ranging from coding to medicine to law to the creative arts. It is owned by people and controlled by people, like any other technology. A shorter description of what AI isn’t: Killer software and robots that will spring to life and decide to murder the human race or otherwise ruin everything, like you see in the movies. An even shorter description of what AI could be: A way to make everything we care about better. Why AI Can Make Everything We Care About Better The most validated core conclusion of social science across many decades and thousands of studies is that human intelligence makes a very broad range of life outcomes better. Smarter people have better outcomes in almost every domain of activity: academic achievement, job performance, occupational status, income, creativity, physical health, longevity, learning new skills, managing complex tasks, leadership, entrepreneurial success, conflict resolution, reading comprehension, financial decision making, understanding others’ perspectives, creative arts, parenting outcomes, and life satisfaction. Further, human intelligence is the lever that we have used for millennia to create the world we live in today: science, technology, math, physics, chemistry, medicine, energy, construction, transportation, communication, art, music, culture, philosophy, ethics, morality. Without the application of intelligence on all these domains, we would all still be living in mud huts, scratching out a meager existence of subsistence farming. Instead we have used our intelligence to raise our standard of living on the order of 10,000X over the last 4,000 years. What AI offers us is the opportunity to profoundly augment human intelligence to make all of these outcomes of intelligence – and many others, from the creation of new medicines to ways to solve climate change to technologies to reach the stars – much, much better from here. AI augmentation of human intelligence has already started – AI is already around us in the form of computer control systems of many kinds, is now rapidly escalating with AI Large Language Models like ChatGPT, and will accelerate very quickly from here – if we let it. In our new era of AI: Every child will have an AI tutor that is infinitely patient, infinitely compassionate, infinitely knowledgeable, infinitely helpful. The AI tutor will be by each child’s side every step of their development, helping them maximize their potential with the machine version of infinite love. Every person will have an AI assistant/coach/mentor/trainer/advisor/therapist that is infinitely patient, infinitely compassionate, infinitely knowledgeable, and infinitely helpful. The AI assistant will be present through all of life’s opportunities and challenges, maximizing every person’s outcomes. Every scientist will have an AI assistant/collaborator/partner that will greatly expand their scope of scientific research and achievement. Every artist, every engineer, every businessperson, every doctor, every caregiver will have the same in their worlds. Every leader of people – CEO, government official, nonprofit president, athletic coach, teacher – will have the same. The magnification effects of better decisions by leaders across the people they lead are enormous, so this intelligence augmentation may be the most important of all. Productivity growth throughout the economy will accelerate dramatically, driving economic growth, creation of new industries, creation of new jobs, and wage growth, and resulting in a new era of heightened material prosperity across the planet. Scientific breakthroughs and new technologies and medicines will dramatically expand, as AI helps us further decode the laws of nature and harvest them for our benefit. The creative arts will enter a golden age, as AI-augmented artists, musicians, writers, and filmmakers gain the ability to realize their visions far faster and at greater scale than ever before. I even think AI is going to improve warfare, when it has to happen, by reducing wartime death rates dramatically. Every war is characterized by terrible decisions made under intense pressure and with sharply limited information by very limited human leaders. Now, military commanders and political leaders will have AI advisors that will help them make much better strategic and tactical decisions, minimizing risk, error, and unnecessary bloodshed. In short, anything that people do with their natural intelligence today can be done much better with AI, and we will be able to take on new challenges that have been impossible to tackle without AI, from curing all diseases to achieving interstellar travel. And this isn’t just about intelligence! Perhaps the most underestimated quality of AI is how humanizing it can be. AI art gives people who otherwise lack technical skills the freedom to create and share their artistic ideas. Talking to an empathetic AI friend really does improve their ability to handle adversity. And AI medical chatbots are already more empathetic than their human counterparts. Rather than making the world harsher and more mechanistic, infinitely patient and sympathetic AI will make the world warmer and nicer. The stakes here are high. The opportunities are profound. AI is quite possibly the most important – and best – thing our civilization has ever created, certainly on par with electricity and microchips, and probably beyond those. The development and proliferation of AI – far from a risk that we should fear – is a moral obligation that we have to ourselves, to our children, and to our future. We should be living in a much better world with AI, and now we can.

So Why The Panic? In contrast to this positive view, the public conversation about AI is presently shot through with hysterical fear and paranoia. We hear claims that AI will variously kill us all, ruin our society, take all our jobs, cause crippling inequality, and enable bad people to do awful things. What explains this divergence in potential outcomes from near utopia to horrifying dystopia? Historically, every new technology that matters, from electric lighting to automobiles to radio to the Internet, has sparked a moral panic – a social contagion that convinces people the new technology is going to destroy the world, or society, or both. The fine folks at Pessimists Archive have documented these technology-driven moral panics over the decades; their history makes the pattern vividly clear. It turns out this present panic is not even the first for AI. Now, it is certainly the case that many new technologies have led to bad outcomes – often the same technologies that have been otherwise enormously beneficial to our welfare. So it’s not that the mere existence of a moral panic means there is nothing to be concerned about. But a moral panic is by its very nature irrational – it takes what may be a legitimate concern and inflates it into a level of hysteria that ironically makes it harder to confront actually serious concerns. And wow do we have a full-blown moral panic about AI right now. This moral panic is already being used as a motivating force by a variety of actors to demand policy action – new AI restrictions, regulations, and laws. These actors, who are making extremely dramatic public statements about the dangers of AI – feeding on and further inflaming moral panic – all present themselves as selfless champions of the public good. But are they? And are they right or wrong?

The Baptists And Bootleggers Of AI Economists have observed a longstanding pattern in reform movements of this kind. The actors within movements like these fall into two categories – “Baptists” and “Bootleggers” – drawing on the historical example of the prohibition of alcohol in the United States in the 1920’s: “Baptists” are the true believer social reformers who legitimately feel – deeply and emotionally, if not rationally – that new restrictions, regulations, and laws are required to prevent societal disaster. For alcohol prohibition, these actors were often literally devout Christians who felt that alcohol was destroying the moral fabric of society. For AI risk, these actors are true believers that AI presents one or another existential risks – strap them to a polygraph, they really mean it. “Bootleggers” are the self-interested opportunists who stand to financially profit by the imposition of new restrictions, regulations, and laws that insulate them from competitors. For alcohol prohibition, these were the literal bootleggers who made a fortune selling illicit alcohol to Americans when legitimate alcohol sales were banned. For AI risk, these are CEOs who stand to make more money if regulatory barriers are erected that form a cartel of government-blessed AI vendors protected from new startup and open source competition – the software version of “too big to fail” banks. A cynic would suggest that some of the apparent Baptists are also Bootleggers – specifically the ones paid to attack AI by their universities, think tanks, activist groups, and media outlets. If you are paid a salary or receive grants to foster AI panic…you are probably a Bootlegger. The problem with the Bootleggers is that they win. The Baptists are naive ideologues, the Bootleggers are cynical operators, and so the result of reform movements like these is often that the Bootleggers get what they want – regulatory capture, insulation from competition, the formation of a cartel – and the Baptists are left wondering where their drive for social improvement went so wrong. We just lived through a stunning example of this – banking reform after the 2008 global financial crisis. The Baptists told us that we needed new laws and regulations to break up the “too big to fail” banks to prevent such a crisis from ever happening again. So Congress passed the Dodd-Frank Act of 2010, which was marketed as satisfying the Baptists’ goal, but in reality was coopted by the Bootleggers – the big banks. The result is that the same banks that were “too big to fail” in 2008 are much, much larger now. So in practice, even when the Baptists are genuine – and even when the Baptists are right – they are used as cover by manipulative and venal Bootleggers to benefit themselves. And this is what is happening in the drive for AI regulation right now. However, it isn’t sufficient to simply identify the actors and impugn their motives. We should consider the arguments of both the Baptists and the Bootleggers on their merits.

AI Risk #1: Will AI Kill Us All? The first and original AI doomer risk is that AI will decide to literally kill humanity. The fear that technology of our own creation will rise up and destroy us is deeply coded into our culture. The Greeks expressed this fear in the Prometheus Myth – Prometheus brought the destructive power of fire, and more generally technology (“techne”), to man, for which Prometheus was condemned to perpetual torture by the gods. Later, Mary Shelley gave us moderns our own version of this myth in her novel Frankenstein, or, The Modern Prometheus, in which we develop the technology for eternal life, which then rises up and seeks to destroy us. And of course, no AI panic newspaper story is complete without a still image of a gleaming red-eyed killer robot from James Cameron’s Terminator films. The presumed evolutionary purpose of this mythology is to motivate us to seriously consider potential risks of new technologies – fire, after all, can indeed be used to burn down entire cities. But just as fire was also the foundation of modern civilization as used to keep us warm and safe in a cold and hostile world, this mythology ignores the far greater upside of most – all? – new technologies, and in practice inflames destructive emotion rather than reasoned analysis. Just because premodern man freaked out like this doesn’t mean we have to; we can apply rationality instead. My view is that the idea that AI will decide to literally kill humanity is a profound category error. AI is not a living being that has been primed by billions of years of evolution to participate in the battle for the survival of the fittest, as animals are, and as we are. It is math – code – computers, built by people, owned by people, used by people, controlled by people. The idea that it will at some point develop a mind of its own and decide that it has motivations that lead it to try to kill us is a superstitious handwave. In short, AI doesn’t want, it doesn’t have goals, it doesn’t want to kill you, because it’s not alive. And AI is a machine – is not going to come alive any more than your toaster will. Now, obviously, there are true believers in killer AI – Baptists – who are gaining a suddenly stratospheric amount of media coverage for their terrifying warnings, some of whom claim to have been studying the topic for decades and say they are now scared out of their minds by what they have learned. Some of these true believers are even actual innovators of the technology. These actors are arguing for a variety of bizarre and extreme restrictions on AI ranging from a ban on AI development, all the way up to military airstrikes on datacenters and nuclear war. They argue that because people like me cannot rule out future catastrophic consequences of AI, that we must assume a precautionary stance that may require large amounts of physical violence and death in order to prevent potential existential risk. My response is that their position is non-scientific – What is the testable hypothesis? What would falsify the hypothesis? How do we know when we are getting into a danger zone? These questions go mainly unanswered apart from “You can’t prove it won’t happen!” In fact, these Baptists’ position is so non-scientific and so extreme – a conspiracy theory about math and code – and is already calling for physical violence, that I will do something I would normally not do and question their motives as well. Specifically, I think three things are going on: First, recall that John Von Neumann responded to Robert Oppenheimer’s famous hand-wringing about his role creating nuclear weapons – which helped end World War II and prevent World War III – with, “Some people confess guilt to claim credit for the sin.” What is the most dramatic way one can claim credit for the importance of one’s work without sounding overtly boastful? This explains the mismatch between the words and actions of the Baptists who are actually building and funding AI – watch their actions, not their words. (Truman was harsher after meeting with Oppenheimer: “Don’t let that crybaby in here again.”) Second, some of the Baptists are actually Bootleggers. There is a whole profession of “AI safety expert”, “AI ethicist”, “AI risk researcher”. They are paid to be doomers, and their statements should be processed appropriately. Third, California is justifiably famous for our many thousands of cults, from EST to the Peoples Temple, from Heaven’s Gate to the Manson Family. Many, although not all, of these cults are harmless, and maybe even serve a purpose for alienated people who find homes in them. But some are very dangerous indeed, and cults have a notoriously hard time straddling the line that ultimately leads to violence and death. And the reality, which is obvious to everyone in the Bay Area but probably not outside of it, is that “AI risk” has developed into a cult, which has suddenly emerged into the daylight of global press attention and the public conversation. This cult has pulled in not just fringe characters, but also some actual industry experts and a not small number of wealthy donors – including, until recently, Sam Bankman-Fried. And it’s developed a full panoply of cult behaviors and beliefs. This cult is why there are a set of AI risk doomers who sound so extreme – it’s not that they actually have secret knowledge that make their extremism logical, it’s that they’ve whipped themselves into a frenzy and really are…extremely extreme. It turns out that this type of cult isn’t new – there is a longstanding Western tradition of millenarianism, which generates apocalypse cults. The AI risk cult has all the hallmarks of a millenarian apocalypse cult. From Wikipedia, with additions by me: “Millenarianism is the belief by a group or movement [AI risk doomers] in a coming fundamental transformation of society [the arrival of AI], after which all things will be changed [AI utopia, dystopia, and/or end of the world]. Only dramatic events [AI bans, airstrikes on datacenters, nuclear strikes on unregulated AI] are seen as able to change the world [prevent AI] and the change is anticipated to be brought about, or survived, by a group of the devout and dedicated. In most millenarian scenarios, the disaster or battle to come [AI apocalypse, or its prevention] will be followed by a new, purified world [AI bans] in which the believers will be rewarded [or at least acknowledged to have been correct all along].” This apocalypse cult pattern is so obvious that I am surprised more people don’t see it. Don’t get me wrong, cults are fun to hear about, their written material is often creative and fascinating, and their members are engaging at dinner parties and on TV. But their extreme beliefs should not determine the future of laws and society – obviously not.

AI Risk #2: Will AI Ruin Our Society? The second widely mooted AI risk is that AI will ruin our society, by generating outputs that will be so “harmful”, to use the nomenclature of this kind of doomer, as to cause profound damage to humanity, even if we’re not literally killed. Short version: If the murder robots don’t get us, the hate speech and misinformation will. This is a relatively recent doomer concern that branched off from and somewhat took over the “AI risk” movement that I described above. In fact, the terminology of AI risk recently changed from “AI safety” – the term used by people who are worried that AI would literally kill us – to “AI alignment” – the term used by people who are worried about societal “harms”. The original AI safety people are frustrated by this shift, although they don’t know how to put it back in the box – they now advocate that the actual AI risk topic be renamed “AI notkilleveryoneism”, which has not yet been widely adopted but is at least clear. The tipoff to the nature of the AI societal risk claim is its own term, “AI alignment”. Alignment with what? Human values. Whose human values? Ah, that’s where things get tricky. As it happens, I have had a front row seat to an analogous situation – the social media “trust and safety” wars. As is now obvious, social media services have been under massive pressure from governments and activists to ban, restrict, censor, and otherwise suppress a wide range of content for many years. And the same concerns of “hate speech” (and its mathematical counterpart, “algorithmic bias”) and “misinformation” are being directly transferred from the social media context to the new frontier of “AI alignment”. My big learnings from the social media wars are: On the one hand, there is no absolutist free speech position. First, every country, including the United States, makes at least some content illegal. Second, there are certain kinds of content, like child pornography and incitements to real world violence, that are nearly universally agreed to be off limits – legal or not – by virtually every society. So any technological platform that facilitates or generates content – speech – is going to have some restrictions. On the other hand, the slippery slope is not a fallacy, it’s an inevitability. Once a framework for restricting even egregiously terrible content is in place – for example, for hate speech, a specific hurtful word, or for misinformation, obviously false claims like “the Pope is dead” – a shockingly broad range of government agencies and activist pressure groups and nongovernmental entities will kick into gear and demand ever greater levels of censorship and suppression of whatever speech they view as threatening to society and/or their own personal preferences. They will do this up to and including in ways that are nakedly felony crimes. This cycle in practice can run apparently forever, with the enthusiastic support of authoritarian hall monitors installed throughout our elite power structures. This has been cascading for a decade in social media and with only certain exceptions continues to get more fervent all the time. And so this is the dynamic that has formed around “AI alignment” now. Its proponents claim the wisdom to engineer AI-generated speech and thought that are good for society, and to ban AI-generated speech and thoughts that are bad for society. Its opponents claim that the thought police are breathtakingly arrogant and presumptuous – and often outright criminal, at least in the US – and in fact are seeking to become a new kind of fused government-corporate-academic authoritarian speech dictatorship ripped straight from the pages of George Orwell’s 1984. As the proponents of both “trust and safety” and “AI alignment” are clustered into the very narrow slice of the global population that characterizes the American coastal elites – which includes many of the people who work in and write about the tech industry – many of my readers will find yourselves primed to argue that dramatic restrictions on AI output are required to avoid destroying society. I will not attempt to talk you out of this now, I will simply state that this is the nature of the demand, and that most people in the world neither agree with your ideology nor want to see you win. If you don’t agree with the prevailing niche morality that is being imposed on both social media and AI via ever-intensifying speech codes, you should also realize that the fight over what AI is allowed to say/generate will be even more important – by a lot – than the fight over social media censorship. AI is highly likely to be the control layer for everything in the world. How it is allowed to operate is going to matter perhaps more than anything else has ever mattered. You should be aware of how a small and isolated coterie of partisan social engineers are trying to determine that right now, under cover of the age-old claim that they are protecting you. In short, don’t let the thought police suppress AI.

AI Risk #3: Will AI Take All Our Jobs? The fear of job loss due variously to mechanization, automation, computerization, or AI has been a recurring panic for hundreds of years, since the original onset of machinery such as the mechanical loom. Even though every new major technology has led to more jobs at higher wages throughout history, each wave of this panic is accompanied by claims that “this time is different” – this is the time it will finally happen, this is the technology that will finally deliver the hammer blow to human labor. And yet, it never happens. We’ve been through two such technology-driven unemployment panic cycles in our recent past – the outsourcing panic of the 2000’s, and the automation panic of the 2010’s. Notwithstanding many talking heads, pundits, and even tech industry executives pounding the table throughout both decades that mass unemployment was near, by late 2019 – right before the onset of COVID – the world had more jobs at higher wages than ever in history. Nevertheless this mistaken idea will not die. And sure enough, it’s back. This time, we finally have the technology that’s going to take all the jobs and render human workers superfluous – real AI. Surely this time history won’t repeat, and AI will cause mass unemployment – and not rapid economic, job, and wage growth – right? No, that’s not going to happen – and in fact AI, if allowed to develop and proliferate throughout the economy, may cause the most dramatic and sustained economic boom of all time, with correspondingly record job and wage growth – the exact opposite of the fear. And here’s why. The core mistake the automation-kills-jobs doomers keep making is called the Lump Of Labor Fallacy. This fallacy is the incorrect notion that there is a fixed amount of labor to be done in the economy at any given time, and either machines do it or people do it – and if machines do it, there will be no work for people to do. The Lump Of Labor Fallacy flows naturally from naive intuition, but naive intuition here is wrong. When technology is applied to production, we get productivity growth – an increase in output generated by a reduction in inputs. The result is lower prices for goods and services. As prices for goods and services fall, we pay less for them, meaning that we now have extra spending power with which to buy other things. This increases demand in the economy, which drives the creation of new production – including new products and new industries – which then creates new jobs for the people who were replaced by machines in prior jobs. The result is a larger economy with higher material prosperity, more industries, more products, and more jobs. But the good news doesn’t stop there. We also get higher wages. This is because, at the level of the individual worker, the marketplace sets compensation as a function of the marginal productivity of the worker. A worker in a technology-infused business will be more productive than a worker in a traditional business. The employer will either pay that worker more money as he is now more productive, or another employer will, purely out of self interest. The result is that technology introduced into an industry generally not only increases the number of jobs in the industry but also raises wages. To summarize, technology empowers people to be more productive. This causes the prices for existing goods and services to fall, and for wages to rise. This in turn causes economic growth and job growth, while motivating the creation of new jobs and new industries. If a market economy is allowed to function normally and if technology is allowed to be introduced freely, this is a perpetual upward cycle that never ends. For, as Milton Friedman observed, “Human wants and needs are endless” – we always want more than we have. A technology-infused market economy is the way we get closer to delivering everything everyone could conceivably want, but never all the way there. And that is why technology doesn’t destroy jobs and never will. These are such mindblowing ideas for people who have not been exposed to them that it may take you some time to wrap your head around them. But I swear I’m not making them up – in fact you can read all about them in standard economics textbooks. I recommend the chapter The Curse of Machinery in Henry Hazlitt’s Economics In One Lesson, and Frederic Bastiat’s satirical Candlemaker’s Petition to blot out the sun due to its unfair competition with the lighting industry, here modernized for our times. But this time is different, you’re thinking. This time, with AI, we have the technology that can replace ALL human labor. But, using the principles I described above, think of what it would mean for literally all existing human labor to be replaced by machines. It would mean a takeoff rate of economic productivity growth that would be absolutely stratospheric, far beyond any historical precedent. Prices of existing goods and services would drop across the board to virtually zero. Consumer welfare would skyrocket. Consumer spending power would skyrocket. New demand in the economy would explode. Entrepreneurs would create dizzying arrays of new industries, products, and services, and employ as many people and AI as they could as fast as possible to meet all the new demand. Suppose AI once again replaces that labor? The cycle would repeat, driving consumer welfare, economic growth, and job and wage growth even higher. It would be a straight spiral up to a material utopia that neither Adam Smith or Karl Marx ever dared dream of. We should be so lucky.

AI Risk #4: Will AI Lead To Crippling Inequality? Speaking of Karl Marx, the concern about AI taking jobs segues directly into the next claimed AI risk, which is, OK, Marc, suppose AI does take all the jobs, either for bad or for good. Won’t that result in massive and crippling wealth inequality, as the owners of AI reap all the economic rewards and regular people get nothing? As it happens, this was a central claim of Marxism, that the owners of the means of production – the bourgeoisie – would inevitably steal all societal wealth from the people who do the actual work – the proletariat. This is another fallacy that simply will not die no matter how often it’s disproved by reality. But let’s drive a stake through its heart anyway. The flaw in this theory is that, as the owner of a piece of technology, it’s not in your own interest to keep it to yourself – in fact the opposite, it’s in your own interest to sell it to as many customers as possible. The largest market in the world for any product is the entire world, all 8 billion of us. And so in reality, every new technology – even ones that start by selling to the rarefied air of high-paying big companies or wealthy consumers – rapidly proliferates until it’s in the hands of the largest possible mass market, ultimately everyone on the planet. The classic example of this was Elon Musk’s so-called “secret plan” – which he naturally published openly – for Tesla in 2006: Step 1, Build [expensive] sports car Step 2, Use that money to build an affordable car Step 3, Use that money to build an even more affordable car …which is of course exactly what he’s done, becoming the richest man in the world as a result. That last point is key. Would Elon be even richer if he only sold cars to rich people today? No. Would he be even richer than that if he only made cars for himself? Of course not. No, he maximizes his own profit by selling to the largest possible market, the world. In short, everyone gets the thing – as we saw in the past with not just cars but also electricity, radio, computers, the Internet, mobile phones, and search engines. The makers of such technologies are highly motivated to drive down their prices until everyone on the planet can afford them. This is precisely what is already happening in AI – it’s why you can use state of the art generative AI not just at low cost but even for free today in the form of Microsoft Bing and Google Bard – and it is what will continue to happen. Not because such vendors are foolish or generous but precisely because they are greedy – they want to maximize the size of their market, which maximizes their profits. So what happens is the opposite of technology driving centralization of wealth – individual customers of the technology, ultimately including everyone on the planet, are empowered instead, and capture most of the generated value. As with prior technologies, the companies that build AI – assuming they have to function in a free market – will compete furiously to make this happen. Marx was wrong then, and he’s wrong now. This is not to say that inequality is not an issue in our society. It is, it’s just not being driven by technology, it’s being driven by the reverse, by the sectors of the economy that are the most resistant to new technology, that have the most government intervention to prevent the adoption of new technology like AI – specifically housing, education, and health care. The actual risk of AI and inequality is not that AI will cause more inequality but rather that we will not allow AI to be used to reduce inequality.

AI Risk #5: Will AI Lead To Bad People Doing Bad Things? So far I have explained why four of the five most often proposed risks of AI are not actually real – AI will not come to life and kill us, AI will not ruin our society, AI will not cause mass unemployment, and AI will not cause an ruinous increase in inequality. But now let’s address the fifth, the one I actually agree with: AI will make it easier for bad people to do bad things. In some sense this is a tautology. Technology is a tool. Tools, starting with fire and rocks, can be used to do good things – cook food and build houses – and bad things – burn people and bludgeon people. Any technology can be used for good or bad. Fair enough. And AI will make it easier for criminals, terrorists, and hostile governments to do bad things, no question. This causes some people to propose, well, in that case, let’s not take the risk, let’s ban AI now before this can happen. Unfortunately, AI is not some esoteric physical material that is hard to come by, like plutonium. It’s the opposite, it’s the easiest material in the world to come by – math and code. The AI cat is obviously already out of the bag. You can learn how to build AI from thousands of free online courses, books, papers, and videos, and there are outstanding open source implementations proliferating by the day. AI is like air – it will be everywhere. The level of totalitarian oppression that would be required to arrest that would be so draconian – a world government monitoring and controlling all computers? jackbooted thugs in black helicopters seizing rogue GPUs? – that we would not have a society left to protect. So instead, there are two very straightforward ways to address the risk of bad people doing bad things with AI, and these are precisely what we should focus on. First, we have laws on the books to criminalize most of the bad things that anyone is going to do with AI. Hack into the Pentagon? That’s a crime. Steal money from a bank? That’s a crime. Create a bioweapon? That’s a crime. Commit a terrorist act? That’s a crime. We can simply focus on preventing those crimes when we can, and prosecuting them when we cannot. We don’t even need new laws – I’m not aware of a single actual bad use for AI that’s been proposed that’s not already illegal. And if a new bad use is identified, we ban that use. QED. But you’ll notice what I slipped in there – I said we should focus first on preventing AI-assisted crimes before they happen – wouldn’t such prevention mean banning AI? Well, there’s another way to prevent such actions, and that’s by using AI as a defensive tool. The same capabilities that make AI dangerous in the hands of bad guys with bad goals make it powerful in the hands of good guys with good goals – specifically the good guys whose job it is to prevent bad things from happening. For example, if you are worried about AI generating fake people and fake videos, the answer is to build new systems where people can verify themselves and real content via cryptographic signatures. Digital creation and alteration of both real and fake content was already here before AI; the answer is not to ban word processors and Photoshop – or AI – but to use technology to build a system that actually solves the problem. And so, second, let’s mount major efforts to use AI for good, legitimate, defensive purposes. Let’s put AI to work in cyberdefense, in biological defense, in hunting terrorists, and in everything else that we do to keep ourselves, our communities, and our nation safe. There are already many smart people in and out of government doing exactly this, of course – but if we apply all of the effort and brainpower that’s currently fixated on the futile prospect of banning AI to using AI to protect against bad people doing bad things, I think there’s no question a world infused with AI will be much safer than the world we live in today.

The Actual Risk Of Not Pursuing AI With Maximum Force And Speed There is one final, and real, AI risk that is probably the scariest at all: AI isn’t just being developed in the relatively free societies of the West, it is also being developed by the Communist Party of the People’s Republic of China. China has a vastly different vision for AI than we do – they view it as a mechanism for authoritarian population control, full stop. They are not even being secretive about this, they are very clear about it, and they are already pursuing their agenda. And they do not intend to limit their AI strategy to China – they intend to proliferate it all across the world, everywhere they are powering 5G networks, everywhere they are loaning Belt And Road money, everywhere they are providing friendly consumer apps like Tiktok that serve as front ends to their centralized command and control AI. The single greatest risk of AI is that China wins global AI dominance and we – the United States and the West – do not. I propose a simple strategy for what to do about this – in fact, the same strategy President Ronald Reagan used to win the first Cold War with the Soviet Union. “We win, they lose.” Rather than allowing ungrounded panics around killer AI, “harmful” AI, job-destroying AI, and inequality-generating AI to put us on our back feet, we in the United States and the West should lean into AI as hard as we possibly can. We should seek to win the race to global AI technological superiority and ensure that China does not. In the process, we should drive AI into our economy and society as fast and hard as we possibly can, in order to maximize its gains for economic productivity and human potential. This is the best way both to offset the real AI risks and to ensure that our way of life is not displaced by the much darker Chinese vision.

What Is To Be Done? I propose a simple plan: Big AI companies should be allowed to build AI as fast and aggressively as they can – but not allowed to achieve regulatory capture, not allowed to establish a government-protect cartel that is insulated from market competition due to incorrect claims of AI risk. This will maximize the technological and societal payoff from the amazing capabilities of these companies, which are jewels of modern capitalism. Startup AI companies should be allowed to build AI as fast and aggressively as they can. They should neither confront government-granted protection of big companies, nor should they receive government assistance. They should simply be allowed to compete. If and as startups don’t succeed, their presence in the market will also continuously motivate big companies to be their best – our economies and societies win either way. Open source AI should be allowed to freely proliferate and compete with both big AI companies and startups. There should be no regulatory barriers to open source whatsoever. Even when open source does not beat companies, its widespread availability is a boon to students all over the world who want to learn how to build and use AI to become part of the technological future, and will ensure that AI is available to everyone who can benefit from it no matter who they are or how much money they have. To offset the risk of bad people doing bad things with AI, governments working in partnership with the private sector should vigorously engage in each area of potential risk to use AI to maximize society’s defensive capabilities. This shouldn’t be limited to AI-enabled risks but also more general problems such as malnutrition, disease, and climate. AI can be an incredibly powerful tool for solving problems, and we should embrace it as such. To prevent the risk of China achieving global AI dominance, we should use the full power of our private sector, our scientific establishment, and our governments in concert to drive American and Western AI to absolute global dominance, including ultimately inside China itself. We win, they lose. And that is how we use AI to save the world. It’s time to build.

Legends and Heroes I close with two simple statements. The development of AI started in the 1940’s, simultaneous with the invention of the computer. The first scientific paper on neural networks – the architecture of the AI we have today – was published in 1943. Entire generations of AI scientists over the last 80 years were born, went to school, worked, and in many cases passed away without seeing the payoff that we are receiving now. They are legends, every one. Today, growing legions of engineers – many of whom are young and may have had grandparents or even great-grandparents involved in the creation of the ideas behind AI – are working to make AI a reality, against a wall of fear-mongering and doomerism that is attempting to paint them as reckless villains. I do not believe they are reckless or villains. They are heroes, every one. My firm and I are thrilled to back as many of them as we can, and we will stand alongside them and their work 100%.

---

01072022 by Lenny:

眼下五大洲各地本已根深蒂固的多维度不平等在疫情的催化下愈发不可逆地恶化；越来越多人在各自的生存处境中摇摇欲坠，成为需要〞激进关怀"的对象。这是不假的事实。我不禁想问勤更豆瓣勤录播客的某些博主们(票圈里有的话，这是给你们的问题）：在当前世界经济制度无法被大规模改进的情况下，作为普通人，我们到底如何以非激进的、遵循社会约定成俗和公共理性的方式关怀身边正在坠落的任何人（不分种族、国籍、宗教、性别身份、教育背景）？如果没有确切答案，我们是否应该重新审视"阶级政治的时代已经翻篇、不应重提"这种自我蒙蔽式的论断？If an"elevated discourse"(consisting in preferred ideologies andetiquettes) is all that you care about, howcan you still associate yourself withconcepts as multi-layered, profound andentrenched as"decency" and "empathy"?

---

02222022 on Twitter:

Venom is an example of a villain who evolved into a hero because he was popular with readers and writers decided to rehabilitate him. By contrast, Magneto evolved into a hero because every decade since Reagan it's harder and harder to pretend he's wrong

  

03122022 by Mesaiocatus:

今天见到周老师，和他聊到读博和打工其实有很多相同之处：都是要先找好老板，然后找 project做（可以是自己找的也可以是老板给的），然后在老板的support 下做，然后老板给你撑腰拿去给更上面的人评估，评估以后给你 credit.

我：但是经常有做了-一个project，做到一半最后发现有坑，技术上实现不了，或者做完了用户metrics不行，，导致最后得不到应有的credit， 所以事先一群人•起做一个evaluation很重要。你们读博的话，老板也会帮你做这种 evaluation 吗？

周老师：做数学其实导师也不知道这个topic最后能不能做出来，可能他有一个感觉，但是他的感觉也不定对。做数学提出一个好的问题，比怎么做要重要得多我：那导师的作用是什么？

周老师：因为我们做的都是没人做过的东西，而且数学都是要靠自己想，所以导师其实对project本身提供不了什么帮助，他对于没人做过的东西也末必会做.也不知道能不能做成。但是导师最重要的价值是，他经常去开会，和业界大佬有很多connection，就知道现在的学术界大佬喜欢什么、流行什么、什么问题容易发。就像你们做产品最终要用户metrics一样，发顶会最重要的是对那几个顶会reviewer的口味。其实他们可能都是傻逼，他们也不知道什么方向更有未来，发不发全凭自己的喜好，但是他们可能可以就这样决定末来十年数学界的走向。

我：所以数学生涯也是市场化的咯？周老师：是的。如果你钻牛角尖，做一个数学界上没人关注的问题，可能就 paper发不出去。导师的作用就是，在做什么问题上给你指点，让你做更受数学界关注的问题，不要把未来的路走窄了。所谓的数学的taste，其实就是对于数学界市场关注倾向的sense。

我：码农经常有 low hanging fruit 的说法，就是个project容易做但是 impact又很大，所以大家都抢着做。但是这样的 project一般只掌握在部分老板的手里，比如不是那个组的就不能做那个东西。学术界也会有这样的事吗？我听说很多暑研，就是老板提出一个比较容易解决的问题，让本科生做了以后洗经历发paper.

周老师：所有暑研都是这样的。因为暑研更重要的是给你一个整体的做research的体验，从选题到怎么写论文怎么 present， 时间就这么短，问题本身当然不能太难。

（我：码农实习好像也是这样）

至于Low hanging fruit 在数学界是很少的，因为所有论文都是公开的一日一个东西可以做.早就被人做完了。老板不会手里留着什么可以做的问题，因为能做他早就自己做出来了。而目关键是如果你要做一个全新的东西，你不知道这个东西会不会变得很重要很有 impact。比如黎曼猜想和哥德巴赫猜想，都是已经提出了很多年，也还没有解决，但是前者养活了无数数学家，有很多人在做解决它的工具，它的衍生问题，后者就无人问津。你不知道大家会对什么topic感兴趣，会有市场

我：感觉做产品也完全是这样，能不能升职取決于产品能不能成功，而产品能不能成功其实不取決于你而是用户，时运

我：感觉工业界的权力流动太快，太不稳定了。比如一个人今天是director，他就有权力决定下面要做那些项目，怎么分配资源等等。明天他可能就不是了，人的power大部分寄托在公司的体系上。学术界的大佬是为什么能当大佬呢？

周老师：就是一直以来的reputation啊，要一直发重要的文章。如果你几十年前发过一篇，以后就不发了，大家就觉得你是个没什么用的老头。

—

@bayesianboy on Twitter, Jul 11, 2023:  

It seems to me that at least half the people who believe in the in-principle prospect of something in the ballpark of our current AI/ML technologies exhibiting agent-like behaviour do so because they have fundamentally misunderstood the point of intentional stance-taking.

If you’ll all allow me a moment of insufferableness I will assume briefly the role of Dennett-exegete. The bulk of Dennett’s life’s work, what he will be remembered for, is his treatment of mental kinds—phenomenal consciousness, free will, teleology, selfhood, theory of mind, intentionality, cognition itself. Mental kinds such as these have long been associated with an ineffable essence only accessible from within; as such, our philosophical and scientific purchase thereon has been stymied. Dennett’s insight, time and again, has been to put forth conceptual analyses of such terms identifying them instead with their outwardly perceptible manifestations, with what is discernible to a third party observer. This serves the crucial function of rendering mental kinds amenable to empirical investigation. As such, Dennett members among those who have been both celebrated for and accused of handing philosophy of mind off to the neuro- and cognitive scientists.

Many have interpreted Dennett’s work, especially his notion of the intentional stance, as implying that, for mental kinds, a simulation of the phenomenon is tantamount to the phenomenon itself. This mistakes the intended upshot of the work. It is not sufficient for identification or assimilation that someone might, in a highly artificial setting, over limited interaction, or with heavily constrained access to the system’s operation, be unable to differentiate it from the phenomenon. To be assimilated under a mental kind concept, a system must reproduce the functional form of the behaviour in question, not mere superficial tokens thereof.

I see also a general proclivity to mistake theses intended as research heuristic, as a definitional ansatz which renders empirical study possible, as statements of deep metaphysical truths, when the empiricist spirit in which they were intended abjures idle metaphysics.

Nothing like AI/ML systems in present form are capable of functional or behavioural replication of mental kinds in the sense necessary to attribute them with mentalistic faculties, even heuristically. The primary concern in the development and adoption of these systems is and will remain—as Dennett has long stressed—how we humans interact with these systems and wield them as tools. The more AI tools are designed to superficially approximate the hallmarks of mentalistic properties, the more limited cognitive agents such as ourselves will over-attribute competence or comprehension to these systems in ways that will have dire consequences for their use.

Ultimately, we do not want “artificial intelligence.” We do not want systems designed to replicate human judgement, behaviour, or labour. We want tools that are amenable to effective human use and which, when used by humans effectively, offer an improvement over the status quo.