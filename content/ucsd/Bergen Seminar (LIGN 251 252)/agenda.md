# January 18: GPT
* Gwern (Sections 2-4) https://www.gwern.net/GPT-3#gpt-3-implications 
* https://github.com/karpathy/minGPT
* https://jalammar.github.io/illustrated-transformer/

# January 25: Scaling laws
https://www.gwern.net/Scaling-hypothesis
https://arxiv.org/abs/2001.08361
https://arxiv.org/abs/2203.15556 https://www.lesswrong.com/posts/6Fpvch8RR29qLEWNH/chinchilla-s-wild-implications

# February 1: Problems in Alignment 
https://arxiv.org/pdf/1906.01820.pdf
https://arxiv.org/abs/1606.06565
[Actual paper: Language models are mesa-optimizers](https://arxiv.org/pdf/2212.10559.pdf)

# February 8: Reinforcement learning from human feedback
https://arxiv.org/abs/2203.02155
https://arxiv.org/abs/2210.10760

# February 15: Alignment continued
https://arxiv.org/abs/2211.03540
https://www.anthropic.com/constitutional.pdf

# March 8:
Measuring Progress on Scalable Oversight for Large Language Models. https://arxiv.org/pdf/2211.03540.pdf
Language Models (Mostly) Know What They Know. https://arxiv.org/pdf/2207.05221.pdf


# March 15: Alignment with Deep Learning
Alignment from Deep Learning Perspectives. https://arxiv.org/pdf/2209.00626.pdf

LIGN 252
---
# Supervising strong learners by amplifying weak experts
https://arxiv.org/pdf/1810.08575.pdf

# April 13: AI Safety via Debate
https://arxiv.org/pdf/1805.00899.pdf

# April 25: 
https://www.lesswrong.com/posts/Br4xDbYu4Frwrb64a/writeup-progress-on-ai-safety-via-debate-1
https://www.lesswrong.com/posts/PJLABqQ962hZEqhdB/debate-update-obfuscated-arguments-problem

# May 10:
https://www.alignmentforum.org/posts/PT8vSxsusqWuN7JXp/my-understanding-of-paul-christiano-s-iterated-amplification

# May 17: Scalable agent alignment via reward modeling: a research direction
https://arxiv.org/pdf/1811.07871.pdf

# May 24: Recursively Summarizing Books with Human Feedback
https://arxiv.org/pdf/2109.10862.pdf

# June 7:
https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/edit, page 1-28
