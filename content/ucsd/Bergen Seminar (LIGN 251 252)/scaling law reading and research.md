Assigned readings:
1. [Gwern's scaling hypothesis](https://www.gwern.net/Scaling-hypothesis)
2. [Kaplan - Scaling Law](https://arxiv.org/pdf/2001.08361.pdf)
3. [Chinchilla](https://arxiv.org/pdf/2203.15556.pdf)
4. [Chinchilla's implication by nostalgebraist](https://www.lesswrong.com/posts/6Fpvch8RR29qLEWNH/chinchilla-s-wild-implications)

Extension:
- Richard Sutton's The Bitter Lesson
- [Beyond Scaling Law (Best Paper at NERUIPS)](https://arxiv.org/pdf/2206.14486.pdf)


Scaling Law of Language Model with size
# [[Chinchilla - April 2022]]
Scaling law but with both data and parameter sizes
- Beyond Scaling Laws: Good data is helpful

# The Bitter Lesson - March 2019
- It is the most effective for AI research to leverage computation than other competing factors
- Most AI research has been conducted as if the computation available to the agent were constant, but over time, more computation becomes available;
- Researching to leverage human knowledge of the domain is important in the short run, but in the long run, the leveraging of computation is what matters;
- In computer chess and the game Go, initial efforts went into avoiding search by taking advantage of human knowledge, but all those efforts proved irrelevant or worse, once search was applied effectively at scale.
- In speech recognition, early competition in the 1970s, statistical methods won out over the human-knowledge-based methods;
	- What I get from this specific piece is we should care less about emulating human in AI research;
- The consistent direction in the field is towards methods that rely less on human knowledge, and use more computation, together with learning on huge training sets.
